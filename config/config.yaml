# Configuration for adversarial attack method evaluation

##############################################################
# ALL CONFIGS BELOW ARE DEFAULT VALUES THAT CAN BE OVERRIDEN 
# IN THE DEMO.PY SCRIPT THROUGH COMMAND LINE ARGUMENTS
##############################################################

# Dataset configuration
dataset:
  name: "imagenet"
  num_images: 1000    # Number of images to evaluate per model
  image_dir: "data"

# Model configuration
models:
  - "resnet18"     # Standard baseline model
  - "resnet50"     # Deeper variant of ResNet
  - "vgg16"        # Standard CNN architecture
  - "efficientnet" # Modern efficient architecture
  - "mobilenet"    # Lightweight architecture

# Evaluation parameters
evaluation:
  save_results: true
  output_dir: "results"

# Attack method configuration
attack:
  method: "CW"  # Options: FGSM, FFGSM, DeepFool, CW, PGD, CG, LBFGS
  norm_types:
    - "Linf" 
    - "L2"
  
  # Method-specific parameters
  params:
    FGSM:
      untargeted:
        loss_fn: "cross_entropy"
        eps_values:
          Linf: [4/255, 8/255]  # Standard values from Goodfellow et al. (2014)
          L2: [0.5, 1.0]        # Standard values for L2 attacks
      targeted:
        loss_fn: "cross_entropy"
        eps_values:
          Linf: [16/255, 32/255]  # Larger values needed for targeted attacks
          L2: [2.0, 3.0]         # Larger perturbations for targeted attacks

    FFGSM:
      untargeted:
        loss_fn: "cross_entropy"
        alpha_linf: 0.04  # 1.25 × 8/255 
        alpha_l2: 0.625   # 1.25 × 0.5 (the L2 epsilon)
        eps_values:
          Linf: [8/255]   # Should achieve ~87.5% success with SSIM > 0.8
          L2: [0.5]       # Should achieve good success with SSIM > 0.7
      targeted:
        loss_fn: "cross_entropy"
        alpha_linf: 0.02  # 1.25 × 16/255
        alpha_l2: 2.5     # 1.25 × 2.0
        eps_values:
          Linf: [16/255, 32/255]  # Larger epsilon for targeted attacks
          L2: [2.0, 3.0]          # Larger epsilon for targeted attacks
    
    DeepFool:
      # DeepFool is typically untargeted only
      untargeted:
        num_classes: 1000          # ImageNet class count
        overshoot_values: [0.02]   # Standard value from Moosavi-Dezfooli et al. (2016)
        steps: 50                  # Reduced from 100 as DeepFool converges quickly
        early_stopping: true       # Enable early stopping to improve efficiency
        top_k_classes: 10          # Only consider top-k classes for efficiency
    
    CW:
      untargeted:
        confidence_values: [0.0]   # Correct
        c_init: 1.0                # Initial c value
        steps: 1000               # Practical compromise (more is better)
        learning_rate: 0.01        # Standard learning rate from original paper
        abort_early: true          # Enable early stopping for efficiency
      targeted:
        confidence_values: [5.0, 20.0]  # Multiple options to try
        c_init: 10.0               # Higher c value for targeted attacks
        steps: 2000                     # More steps for targeted
        learning_rate: 0.01        # Same learning rate
        abort_early: true          # Enable early stopping for efficiency
    
    CG:
      untargeted:
        loss_fn: "cross_entropy"
        n_iter: 50                      # Parameter name matches our implementation
        beta_method: "HS"               # Changed to match our implementation (HS = Hestenes-Stiefel)
        restart_interval: 10            # Renamed to match our implementation
        tv_lambda: 0.05                 # Added regularization parameter
        color_lambda: 0.05              # Added regularization parameter
        perceptual_lambda: 0.05         # Added regularization parameter
        rand_init: true                 # Random initialization within epsilon ball
        fgsm_init: true                 # Added FGSM initialization
        adaptive_restart: true          # Added adaptive restart parameter
        early_stopping: true            # Stop when attack succeeds
        strict_epsilon_constraint: true # Added parameter to strictly enforce epsilon
        eps_values:
          Linf: [8/255, 16/255]         # Common values from adversarial literature
          L2: [1.0, 2.0]                # Standard L2 bounds
      targeted:
        loss_fn: "cross_entropy"
        n_iter: 100                     # More iterations for targeted attacks
        beta_method: "HS"               # Hestenes-Stiefel formula
        restart_interval: 15            # Less frequent restarts for targeted
        tv_lambda: 0.03                 # Lower regularization for targeted attacks
        color_lambda: 0.03              # Lower regularization for targeted attacks 
        perceptual_lambda: 0.03         # Lower regularization for targeted attacks
        rand_init: true                 # Random initialization
        fgsm_init: true                 # FGSM initialization
        adaptive_restart: true          # Adaptive restart for better convergence
        early_stopping: true            # Stop when attack succeeds
        strict_epsilon_constraint: true # Strictly enforce epsilon constraint
        eps_values:
          Linf: [16/255, 32/255]        # Larger perturbations for targeted
          L2: [2.0, 4.0]                # Larger L2 bounds for targeted

    LBFGS:
      untargeted:
        loss_fn: "cross_entropy"
        n_iterations: 50           # Standard from Zhu et al. (2019)
        history_size: 10           # Common value for L-BFGS memory
        line_search_fn: "armijo"   # Efficient line search
        max_line_search: 15        # Balance between precision and speed
        initial_step: 3.0          # Aggressive initial step size
        rand_init: true           # Random initialization
        init_std: 0.05            # Standard deviation for initialization
        grad_scale: 1.0           # No gradient scaling
        early_stopping: true      # Stop when attack succeeds
        eps_values:
          Linf: [8/255, 16/255]   # Standard bounds from literature
          L2: [1.0, 2.0]          # Common L2 bounds
      targeted:
        loss_fn: "cross_entropy"
        n_iterations: 100          # More iterations for targeted attacks
        history_size: 15           # Larger memory for better Hessian approximation
        line_search_fn: "strong_wolfe" # More precise line search
        max_line_search: 20        # More line search iterations
        initial_step: 2.0          # More conservative step size
        rand_init: true           # Random initialization
        init_std: 0.01            # Smaller initialization
        grad_scale: 1.5           # Slightly aggressive updates
        early_stopping: true      # Stop when attack succeeds
        eps_values:
          Linf: [16/255, 32/255]  # Larger bounds for targeted
          L2: [2.0, 4.0]          # Larger L2 bounds

    PGD:
      # PGD variants for testing different configurations
      variants:
        basic:
          name: "PGD (Basic)"
          params:
            norm: "Linf"
            n_iterations: 40
            step_size: 2/255
            loss_fn: "cross_entropy"
            rand_init: true
            early_stopping: true
        
        momentum:
          name: "PGD (Margin)"
          params:
            norm: "Linf"
            n_iterations: 40
            step_size: 2/255
            loss_fn: "margin"
            rand_init: true
            early_stopping: true
        
        l2:
          name: "PGD (L2)"
          params:
            norm: "L2"
            n_iterations: 40
            step_size: 0.1
            loss_fn: "cross_entropy"
            rand_init: true
            early_stopping: true
      
      untargeted:
        loss_fn: "margin"         # Better loss function than cross_entropy
        n_iterations: 40           # From Madry et al. (2017)
        step_size_linf: 2/255      # Step size (standard practice)
        step_size_l2: 0.1          # Step size for L2 attacks
        rand_init: true            # Random initialization (important for PGD)
        early_stopping: true       # Stop when attack succeeds
        eps_values:
          Linf: [8/255]           # Standard from Madry et al. (2017)
          L2: [1.0, 2.0]          # Common L2 bounds
      targeted:
        loss_fn: "carlini_wagner"  # Better for targeted attacks
        n_iterations: 100          # More iterations for targeted
        step_size_linf: 2/255      # Same step size ratio
        step_size_l2: 0.15         # Slightly larger for targeted
        rand_init: true            # Random initialization
        early_stopping: true       # Stop when attack succeeds
        eps_values:
          Linf: [16/255, 32/255]  # Larger eps for targeted
          L2: [2.0, 3.0]          # Larger L2 bounds

# Target method configuration for targeted attacks
target_methods:
  random:
    description: "Select random target classes"
  least-likely:
    description: "Select least-likely predicted classes (hardest targets)"

# Tables configuration for reporting results
tables:
  table1:
    title: "Success Rates (%)"
    description: "Attack success rates per model"
  
  table2:
    title: "Perturbation Metrics" 
    description: "Measures of perturbation size and quality"
    metrics: ["l2_norm", "linf_norm", "ssim"]
  
  table3:
    title: "Computational Requirements"
    description: "Performance metrics for computational efficiency"
    metrics: ["iterations", "gradient_calls", "runtime"]
